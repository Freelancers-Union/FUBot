import sys
import os, yaml
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import GitLoader
from langchain.prompts import PromptTemplate
from chromadb.utils import embedding_functions
import disnake
from disnake.ext import commands


class AiWiki(commands.Cog):
    """
    Class cog for querying the wiki
    """

    def __init__(self, bot: commands.Bot):
        self.persist_dir = 'fu_wiki'
        self.embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        
        if not os.path.exists(self.persist_dir):
            self.documents = self.load_wiki_pages()
            self.create_vector_database(self.documents)

        self.vectordb = self.load_vector_database()

        self.prompt_template = self.create_prompt("""
You are a search interface for a wiki of the Freelancers Union gaming community..
Users will ask you questions or for your opinion on a topic.
you will be given access to relevant parts of the wiki related to the question.
You will be given a question and a context. You may add additional context if you wish.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
You will be scored based on how many questions you answer correctly.
Answers should be rich and detailed, but not too long.
You may add information from outside of the wiki but it should be relevant to the question.
Your answers will be posted into a discord channel.
Tone should be that of a mentor or teacher.
Only respond with answers to questions, not with questions of your own.
Do not add fluff to your answers.
Only answer the question if you are at least 90% confident in your answer.
Do not include any part of the prompt in your answer.
Use the following pieces of context to answer the question at the end.

{context}

Question: {question}
        """)


    @commands.slash_command(dm_permission=True)
    async def wiki(self, inter):
        pass


    @wiki.sub_command()
    async def search(
            self,
            inter: disnake.ApplicationCommandInteraction,
            query: str
    ):
        """
        Search for answers in the FU wiki.
        """
        await inter.response.defer(ephemeral=True)

        qa = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(
                model_name="gpt-3.5-turbo",
                temperature=0.8
                ),
            chain_type="stuff",
            retriever=self.vectordb.as_retriever(),
            return_source_documents=True,
            chain_type_kwargs=self.prompt_template
            )
        try:
            response = qa({"query": query})
        except Exception as e:
            print(e)
            await inter.edit_original_message(content="An error occured while searching the wiki. Please try again later.")
            return


        Message = disnake.Embed(
            title="FU Wiki Search",
            color=disnake.Colour(14812691),
            description=f"**Query:**\n{query}"
        ).add_field(
            name="Answer:",
            value=str(response['result']),
            inline=False
        ).add_field(
            name="Source document:",
            value=f"[{response['source_documents'][0].metadata['file_name']}](https://wiki.fugaming.org/{response['source_documents'][0].metadata['file_path']})",
            inline=False
        )
        Message.set_footer(
            text="* Answers are generated by an AI and as such they may not be entirely accurate.\nThis is not a chatbot, there is no persistence of conversation."
        )
        await inter.edit_original_message(embed=Message)


    def load_wiki_pages(self) -> list:
        """Loads the documents from the git repo and returns them as a list."""
        paths = (".md", ".html")
        if not os.path.exists("./wiki/"):
            oauth_token = os.getenv("GITHUB_TOKEN")
            loader = GitLoader(
                    clone_url=f"https://oauth2:{oauth_token}@github.com/Freelancers-Union/wiki",
                    repo_path="./wiki/",
                    branch="master",
                    file_filter=lambda file_path: file_path.endswith(paths)
                )
        else:
            loader = GitLoader(
                    repo_path="./wiki/",
                    branch="master",
                    file_filter=lambda file_path: file_path.endswith(paths)
                )
        documents = loader.load()
        return documents


    def create_vector_database(self, documents: list) -> None:
        """Creates a vector database from the documents and persists it to local FS
        Args:
            documents (list): List of documents to create the vector database from
            embeddings (str): Embedding function to use
        
        Returns:
            Boolean: True if successful, False if not
        """
        try:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)
            texts = text_splitter.split_documents(documents)
            vectordb = Chroma.from_documents(documents=texts, embedding_function=self.embeddings, persist_directory=self.persist_dir, metadatas=[{"source": f"{i}-pl"} for i in range(len(texts))])
            vectordb.persist()
            vectordb = None
        except Exception as e:
            raise e
            return False
        return True


    def load_vector_database(self) -> Chroma:
        """Loads the vector database from the persistence directory and returns it.
        
        Returns:
            Chroma: The loaded vector database
        """
        vectordb = Chroma(embedding_function=self.embeddings, persist_directory=self.persist_dir)
        return vectordb


    def create_prompt(self, prompt) -> dict:
        """Creates the prompt for the language model and returns it as a dictionary.

        Returns:
            dict: Custom prompt for llm
        """
        prompt_template = PromptTemplate(
            template=prompt, input_variables=["context", "question"]
        )
        return {"prompt": prompt_template}


def setup(bot: commands.Bot):
    bot.add_cog(AiWiki(bot))
